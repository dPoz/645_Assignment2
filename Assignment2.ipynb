{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7dc17d8-f4ac-4cf2-b38f-3e71a629d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available True\n",
      "CUDA version 12.4\n",
      "cuDNN enabled True\n",
      "cuDNN version 90100\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Torchvision version: 0.21.0+cu124\n",
      "Device 0 name: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torchvision\n",
    "from torchvision import datasets as ds, transforms, models \n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models import resnet18, mobilenet_v2, MobileNet_V2_Weights\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, AdamW, AutoProcessor, get_scheduler, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/poz/Notebooks/645_Assignment2')\n",
    "# from customScript import customScript\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import ImageOps, Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import re\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print('CUDA available', torch.cuda.is_available())\n",
    "print('CUDA version', torch.version.cuda)\n",
    "print('cuDNN enabled', torch.backends.cudnn.enabled)\n",
    "print('cuDNN version', torch.backends.cudnn.version())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "\n",
    "n_cuda_devices = torch.cuda.device_count()\n",
    "for i in range(n_cuda_devices):\n",
    "  print(f'Device {i} name:', torch.cuda.get_device_name(i))\n",
    "\n",
    "batch_size = 64\n",
    "image_resize = 224\n",
    "num_workers = 8\n",
    "num_epochs = 10\n",
    "max_len = 24\n",
    "best_loss = 1e+10\n",
    "learning_rate = 2e-5\n",
    "stats = (torch.tensor([0.4482, 0.4192, 0.3900]), torch.tensor([0.2918, 0.2796, 0.2709]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "715d148a-6571-4a8d-bf38-fe454604bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get thge statistics of a dataset\n",
    "def get_dataset_stats(data_loader):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    nb_samples = 0.\n",
    "    for data in data_loader:\n",
    "        data = data[0] # Get the images to compute the statistics\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "    return mean,std\n",
    "\n",
    "# Modify the imshow function to ensure stats are on the same device as the image\n",
    "def imshow(img, stats):\n",
    "    mean = stats[0].view(3, 1, 1).to(img.device)  # Move mean to the same device as img\n",
    "    std = stats[1].view(3, 1, 1).to(img.device)   # Move std to the same device as img\n",
    "    img = img * std + mean\n",
    "    npimg = img.cpu().numpy()  # Convert the tensor back to numpy after moving it to CPU\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Test function\n",
    "def test_model(model, dataloader):\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Define the model\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DistilBERTClassifier, self).__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(self.distilbert.config.hidden_size, num_classes)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        output = self.drop(pooled_output[:,0])\n",
    "        return self.out(output)\n",
    "\n",
    "# Extract text from file names as well as labels\n",
    "def read_text_files_with_labels(path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    class_folders = sorted(os.listdir(path))\n",
    "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            file_names = os.listdir(class_path)\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(class_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
    "                    text = file_name_no_ext.replace('_', ' ')\n",
    "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
    "                    texts.append(text_without_digits)\n",
    "                    labels.append(label_map[class_name])\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "# Define your dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dataset, tokenizer, max_len):\n",
    "        self.image_dataset = image_dataset\n",
    "        self.class_names = image_dataset.classes\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        #self.label_to_class_map = {i: self.class_names[i] for i in range(len(self.class_names))}\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_dataset[idx]\n",
    "        image_path = self.image_dataset.samples[idx][0]\n",
    "        text = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        text = re.sub(r'\\d+', '', text).replace('_', ' ')\n",
    "        #label = self.label_to_class_map[label]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "# Define training function\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            output = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(output, 1)\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)  # Assuming input_ids are in the batch\n",
    "            attention_mask = batch['attention_mask'].to(device)  # Assuming attention_mask is in the batch\n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            # Convert predictions to CPU and append to the list\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "class MultiInputModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiInputModel, self).__init__()\n",
    "        # Image model\n",
    "        self.image_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        num_features = self.image_model.classifier[1].in_features\n",
    "        self.image_model.classifier[1] = nn.Identity()  # Removing final classifier\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = True  # Fine-tune the image model\n",
    "        # Text model\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 512)  # Reducing output size\n",
    "        for param in self.text_model.parameters():\n",
    "            param.requires_grad = True  # Fine-tune the text model\n",
    "        # Combining both image and text features\n",
    "        self.fc1 = nn.Linear(num_features + 512, 512)  # Merging both feature vectors\n",
    "        self.fc2 = nn.Linear(512, 256)  # Reduced size\n",
    "        self.fc3 = nn.Linear(256, 64)  # Reduced size\n",
    "        self.fc4 = nn.Linear(64, num_classes)  # Output layer\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)  # Batch Normalization after fc1\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)  # Batch Normalization after fc2\n",
    "        self.batch_norm3 = nn.BatchNorm1d(64)  # Batch Normalization after fc3\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        image_features = self.image_model.features(image)\n",
    "        image_features = image_features.mean([2, 3])  # Global Average Pooling\n",
    "        # Text features\n",
    "        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        text_features = text_features[:, 0, :]  # Use [CLS] token for classification\n",
    "        text_features = self.text_fc(text_features)  # Reduce the dimension\n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        # Classifier with Batch Normalization, Activation functions, and Dropout\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.batch_norm1(x)  # Batch Normalization after fc1\n",
    "        x = F.relu(x)  # ReLU activation after batch normalization\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)  # Batch Normalization after fc2\n",
    "        x = F.relu(x)  # ReLU activation after batch normalization\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.batch_norm3(x)  # Batch Normalization after fc2\n",
    "        x = F.relu(x)  # ReLU activation after batch normalization\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)  # No activation for final output layer\n",
    "        return x\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, image_dataset, texts, labels, tokenizer, max_len):\n",
    "        self.image_dataset = image_dataset\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_dataset[idx]\n",
    "        # image = torch.clamp(image, 0, 1)\n",
    "        # imshow(image, stats)\n",
    "        label = self.labels[idx]\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4a3666-8e5d-40d6-aece-3af183802cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Black', 'Blue', 'Green', 'TTR']\n",
      "Train set: 11648\n",
      "Val set: 1824\n",
      "Test set: 3456\n"
     ]
    }
   ],
   "source": [
    "transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((232, 232), interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.RandomCrop(image_resize),\n",
    "        transforms.Resize((image_resize, image_resize)),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomAffine(0.01),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((image_resize, image_resize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((image_resize, image_resize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = r\"/home/poz/garbage_data\"\n",
    "train_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Train\")\n",
    "val_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Val\")\n",
    "test_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Test\")\n",
    "\n",
    "text_train, labels_train = read_text_files_with_labels(train_dir)\n",
    "text_val, labels_val = read_text_files_with_labels(val_dir)\n",
    "text_test, labels_test = read_text_files_with_labels(test_dir)\n",
    "\n",
    "datasets = {\"train\": ds.ImageFolder(train_dir, transform=transform[\"train\"]),\n",
    "            \"val\": ds.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
    "            \"test\": ds.ImageFolder(test_dir, transform=transform[\"test\"])}\n",
    "\n",
    "class_names = datasets['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "datasets = {\"train\": MultiModalDataset(datasets['train'], text_train, labels_train, tokenizer, max_len),\n",
    "            \"val\": MultiModalDataset(datasets['val'], text_val, labels_val, tokenizer, max_len),\n",
    "            \"test\": MultiModalDataset(datasets['test'], text_test, labels_test, tokenizer, max_len)}\n",
    "\n",
    "dataloaders = {\"train\": DataLoader(datasets[\"train\"], batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True),\n",
    "               \"val\": DataLoader(datasets[\"val\"], batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True),\n",
    "               \"test\": DataLoader(datasets[\"test\"], batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)}\n",
    "\n",
    "print(\"Train set:\", len(dataloaders['train'])*batch_size)\n",
    "print(\"Val set:\", len(dataloaders['val'])*batch_size)\n",
    "print(\"Test set:\", len(dataloaders['test'])*batch_size)\n",
    "\n",
    "# # Comopute the statistics of the train set\n",
    "# stats = get_dataset_stats(train_loader)\n",
    "# print(\"Train stats:\", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d552eb0-271a-4c2c-a74d-fde23c68bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiInputModel(num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4be412b1-81ba-4500-ba8c-dd0a124dee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.8549\n",
      "Epoch 1/10, Val Loss: 0.5200, Val Accuracy: 0.8700\n",
      "Learning Rate: 0.000020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m], criterion, device)\n",
      "Cell \u001b[0;32mIn[11], line 109\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    107\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    108\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 109\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, dataloaders['train'], optimizer, criterion, device)\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}')\n",
    "    val_loss, val_accuracy = evaluate(model, dataloaders['val'], criterion, device)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # Step the scheduler after each epoch\n",
    "    scheduler.step()\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()[0]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929eccd-864d-4caf-ab9d-307b85195eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "# labels_test = np.array(list(map(lambda x: x['label'].item(), datasets['test'])))\n",
    "# Evaluation\n",
    "test_predictions = np.array(predict(model, dataloaders['test'], device))\n",
    "print(f\"Accuracy: {(test_predictions == labels_test).sum()/len(labels_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0646f7f-7a76-4a78-b705-f7bfebea86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels_test, test_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4539f-9e8c-4e1d-981c-5e9455976488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
