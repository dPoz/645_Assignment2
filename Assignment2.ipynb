{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc17d8-f4ac-4cf2-b38f-3e71a629d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torchvision\n",
    "from torchvision import datasets as ds, transforms, models \n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models import resnet18, mobilenet_v2, MobileNet_V2_Weights\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, AdamW, AutoProcessor, get_scheduler, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import ImageOps, Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import re\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = \"0\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "print('CUDA available', torch.cuda.is_available())\n",
    "print('CUDA version', torch.version.cuda)\n",
    "print('cuDNN enabled', torch.backends.cudnn.enabled)\n",
    "print('cuDNN version', torch.backends.cudnn.version())\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "\n",
    "n_cuda_devices = torch.cuda.device_count()\n",
    "for i in range(n_cuda_devices):\n",
    "  print(f'Device {i} name:', torch.cuda.get_device_name(i))\n",
    "\n",
    "batch_size = 32\n",
    "image_resize = 224\n",
    "num_workers = 8\n",
    "num_epochs = 20\n",
    "max_len = 24\n",
    "learning_rate = 2e-5\n",
    "stats = (torch.tensor([0.4482, 0.4192, 0.3900]), torch.tensor([0.2918, 0.2796, 0.2709]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24e054-eab6-48d6-9c53-161a7e20b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modify the imshow function to ensure stats are on the same device as the image\n",
    "def imshow(img, stats):\n",
    "    mean = stats[0].view(3, 1, 1).to(img.device)\n",
    "    std = stats[1].view(3, 1, 1).to(img.device)\n",
    "    img = img * std + mean\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Extract text from file names as well as labels\n",
    "def read_text_files_with_labels(path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    class_folders = sorted(os.listdir(path))\n",
    "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            file_names = os.listdir(class_path)\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(class_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
    "                    text = file_name_no_ext.replace('_', ' ')\n",
    "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
    "                    texts.append(text_without_digits)\n",
    "                    labels.append(label_map[class_name])\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, image_dataset, texts, labels, tokenizer, max_len):\n",
    "        self.image_dataset = image_dataset\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_dataset[idx]\n",
    "        label = self.labels[idx]\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686af86-484b-452b-89c6-99283972db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images, input_ids, attention_mask)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "    \n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            output = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(output, 1)\n",
    "            total_correct += torch.sum(preds == labels).item()\n",
    "            total_samples += labels.size(0)\n",
    "    accuracy = 100*total_correct / total_samples\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "def predictALL(model, dataloader, device, class_names):\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "    model.eval()\n",
    "    showFirstTenMissClassed = -1\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating Test\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            for label, prediction, image in zip(labels, preds, images):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                if label != prediction:\n",
    "                    if showFirstTenMissClassed >= 0:\n",
    "                        print(f\"This is classed as: {class_names[label]}\\nThe model predicted class: {class_names[prediction]}\")\n",
    "                        imshow(image, stats)\n",
    "                        showFirstTenMissClassed -= 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "    test_accuracy = 100-(100*(sum(total_pred.values())-sum(correct_pred.values()))/sum(total_pred.values()))\n",
    "    print(f'Test accuracy for all classes: {test_accuracy:.2f}%')\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.2f}%')\n",
    "    return test_accuracy\n",
    "\n",
    "def epochLoop(model, dataloaders, optimizer, criterion, device, class_names, num_epochs):\n",
    "    best_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    best_test_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nStarted Training Loop =\", datetime.now().strftime(f\"%H:%M:%S\"))\n",
    "        train_loss = train(model, dataloaders['train'], optimizer, criterion, device)\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}\\n')\n",
    "        print(f\"Started Validaiton Loop =\", datetime.now().strftime(f\"%H:%M:%S\"))\n",
    "        val_loss, val_accuracy = evaluate(model, dataloaders['val'], criterion, device)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        print(f\"\\nStarted Calculating Test Accuracy =\", datetime.now().strftime(f\"%H:%M:%S\"))\n",
    "        test_accuracy = predictALL(model, dataloaders['test'], device, class_names)\n",
    "        if val_loss >= best_loss*1.5 or val_accuracy <= best_val_accuracy*0.95 or test_accuracy <= best_test_accuracy*0.95 :\n",
    "            print(f\"\\nValidation error grew by 50%, or validation accuracy dropped by 5%, or test accuracy dropped by 5%, so stopped training.\")\n",
    "            break\n",
    "        if val_loss <= best_loss or best_val_accuracy <= val_accuracy:\n",
    "            best_loss = val_loss\n",
    "            best_val_accuracy = val_accuracy\n",
    "        if best_test_accuracy <= test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), f'best_model.pth')\n",
    "            print(f\"\\nThe model has been saved!\")\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c7637-cab2-41c4-b9cc-41f1973a2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiInputModel, self).__init__()\n",
    "        # Image model\n",
    "        self.image_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        num_features = self.image_model.classifier[1].in_features\n",
    "        self.image_model.classifier[1] = nn.Identity()\n",
    "        # Adding additional convolutional and pooling layers to image model\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_features, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Text model\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 768)\n",
    "        # Combining both image and text features\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "        self.fc4 = nn.Linear(16, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # Set requires_grad = True for all parameters in MobileNetV2 and DistilBERT to fine-tune them\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = True  # Allow training of image model\n",
    "        for param in self.text_model.parameters():\n",
    "            param.requires_grad = True  # Allow training of text model\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        image_features = self.image_model.features(image)\n",
    "        image_features = self.pool1(F.relu(self.conv1(image_features)))\n",
    "        image_features = self.pool2(F.relu(self.conv2(image_features)))\n",
    "        image_features = image_features.mean([2, 3])  # Global Average Pooling\n",
    "        # Text features\n",
    "        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        text_features = text_features[:, 0, :]  # Use [CLS] token for classification\n",
    "        text_features = self.text_fc(text_features)\n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        # Classifier with Activation functions, and Dropout\n",
    "        combined_features = self.fc1(combined_features)\n",
    "        combined_features = F.relu(combined_features)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        combined_features = self.fc2(combined_features)\n",
    "        combined_features = F.relu(combined_features)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        combined_features = self.fc3(combined_features)\n",
    "        combined_features = F.relu(combined_features)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        combined_features = self.fc4(combined_features)\n",
    "        combined_features = F.relu(combined_features)\n",
    "        return combined_features\n",
    "        \n",
    "# Define the multi model model \n",
    "class MultiInputModel_2(nn.Module): # 89.19%, 7th epoch\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiInputModel, self).__init__()\n",
    "        # Image model\n",
    "        self.image_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        num_features = self.image_model.classifier[1].in_features\n",
    "        self.image_conv1 = nn.Conv2d(in_channels=num_features, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.image_pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.image_conv2 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.image_pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.image_fc1 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Text model\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.text_fc1 = nn.Linear(self.text_model.config.hidden_size, num_classes)\n",
    "\n",
    "        self.fc1 = nn.Linear(num_classes+num_classes, num_classes)\n",
    "\n",
    "        # Set requires_grad = True for all parameters in MobileNetV2 and DistilBERT to fine-tune them\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = True  # Allow training of image model\n",
    "        for param in self.text_model.parameters():\n",
    "            param.requires_grad = True  # Allow training of text model\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        image_features = self.image_model.features(image)\n",
    "        image_features = self.image_pool1(F.relu(self.image_conv1(image_features)))\n",
    "        image_features = self.image_pool2(F.relu(self.image_conv2(image_features)))\n",
    "        image_features = image_features.mean([2, 3])  # Global Average Pooling\n",
    "        image_features = self.image_fc1(image_features)\n",
    "\n",
    "        # Text features\n",
    "        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        text_features = text_features[:, 0, :]\n",
    "        text_features = self.text_fc1(text_features)\n",
    "        \n",
    "        # Combine image and text features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        \n",
    "        combined_features = self.fc1(combined_features)\n",
    "        combined_features = F.relu(combined_features)\n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a3666-8e5d-40d6-aece-3af183802cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((232, 232), interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.RandomCrop(image_resize),\n",
    "        transforms.Resize((image_resize, image_resize)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((image_resize, image_resize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((image_resize, image_resize)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = r\"/home/poz/garbage_data\"\n",
    "train_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Train\")\n",
    "val_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Val\")\n",
    "test_dir = os.path.join(data_dir, \"CVPR_2024_dataset_Test\")\n",
    "\n",
    "text_train, labels_train = read_text_files_with_labels(train_dir)\n",
    "text_val, labels_val = read_text_files_with_labels(val_dir)\n",
    "text_test, labels_test = read_text_files_with_labels(test_dir)\n",
    "\n",
    "datasets = {\"train\": ds.ImageFolder(train_dir, transform=transform[\"train\"]),\n",
    "            \"val\": ds.ImageFolder(val_dir, transform=transform[\"val\"]),\n",
    "            \"test\": ds.ImageFolder(test_dir, transform=transform[\"test\"])}\n",
    "\n",
    "class_names = datasets['train'].classes\n",
    "print(class_names)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "datasets = {\"train\": MultiModalDataset(datasets['train'], text_train, labels_train, tokenizer, max_len),\n",
    "            \"val\": MultiModalDataset(datasets['val'], text_val, labels_val, tokenizer, max_len),\n",
    "            \"test\": MultiModalDataset(datasets['test'], text_test, labels_test, tokenizer, max_len)}\n",
    "\n",
    "dataloaders = {\"train\": DataLoader(datasets[\"train\"], batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True),\n",
    "               \"val\": DataLoader(datasets[\"val\"], batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True),\n",
    "               \"test\": DataLoader(datasets[\"test\"], batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)}\n",
    "\n",
    "print(\"Train set:\", len(dataloaders['train'])*batch_size)\n",
    "print(\"Val set:\", len(dataloaders['val'])*batch_size)\n",
    "print(\"Test set:\", len(dataloaders['test'])*batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff37e8-2f3e-4463-8fb3-2da0c0803dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiInputModel(num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d8f48-3432-4f82-9871-d7aa7954e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochLoop(model, dataloaders, optimizer, criterion, device, class_names, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929eccd-864d-4caf-ab9d-307b85195eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_accuracy = predictALL(model, dataloaders['test'], device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0646f7f-7a76-4a78-b705-f7bfebea86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels_test, test_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
